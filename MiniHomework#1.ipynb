{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 机器学习中的优化  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 问题1\n",
    "假设我们有训练数据$D=\\{(\\mathbf{x}_1,y_1),...,(\\mathbf{x}_n,y_n)\\}$, 其中$(\\mathbf{x}_i,y_i)$为每一个样本，而且$\\mathbf{x}_i$是样本的特征并且$\\mathbf{x}_i\\in \\mathcal{R}^D$, $y_i$代表样本数据的标签（label）, 取值为$0$或者$1$. 在逻辑回归中，模型的参数为$(\\mathbf{w},b)$。对于向量，我们一般用粗体来表达。 为了后续推导的方便，可以把b融入到参数w中。 这是参数$w$就变成 $w=(w_0, w_1, .., w_D)$，也就是前面多出了一个项$w_0$, 可以看作是b，这时候每一个$x_i$也需要稍作改变可以写成 $x_i = [1, x_i]$，前面加了一个1。稍做思考应该能看出为什么可以这么写。\n",
    "\n",
    "请回答以下问题。请用Markdown自带的Latex来编写。\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  (a) ```编写逻辑回归的目标函数```\n",
    "请写出目标函数（objective function）, 也就是我们需要\"最小化\"的目标（也称之为损失函数或者loss function)，不需要考虑正则。 把目标函数表示成最小化的形态，另外把$\\prod_{}^{}$转换成$\\log \\sum_{}^{}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$L(w)=-\\sum_{i=1}^n[y_ilog\\sigma(\\vec{w}^T\\vec{x_i})+(1-y_i)log[1-\\sigma(\\vec{w}^T\\vec{x_i})]]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  (b) ```求解对w的一阶导数```\n",
    "为了做梯度下降法，我们需要对参数$w$求导，请把$L(w)$对$w$的梯度计算一下："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\frac{\\partial L(w)}{\\partial w}=-\\sum_{i=1}^n[y_i\\frac{x_ie^{-\\vec{w}^T\\vec{x_i}}}{1+e^{-\\vec{w}^T\\vec{x_i}}}+(1-y_i)\\frac{-\\vec{x_i}}{1+e^{-\\vec{w}^T\\vec{x_i}}}]=-\\sum_{i=1}^n[y_i-\\sigma(\\vec{w}^T\\vec{x_i})]\\vec{x_i}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  (c) ```求解对w的二阶导数```\n",
    "在上面结果的基础上对$w$求解二阶导数，也就是再求一次导数。 这个过程需要回忆一下线性代数的部分 ^^。 参考： matrix cookbook: https://www.math.uwaterloo.ca/~hwolkowi/matrixcookbook.pdf, 还有 Hessian Matrix。 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\frac{\\partial^2 L(w)}{\\partial^2 w}=\\sum_{i=1}^n\\frac{e^{-\\vec{w}^T\\vec{x_i}}}{(1+e^{-\\vec{w}^T\\vec{x_i}})^2}\\vec{x_i}\\vec{x_i}^T$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (d) ```证明逻辑回归目标函数是凸函数```\n",
    "试着证明逻辑回归函数是凸函数。假设一个函数是凸函数，我们则可以得出局部最优解即为全局最优解，所以假设我们通过随机梯度下降法等手段找到最优解时我们就可以确认这个解就是全局最优解。证明凸函数的方法有很多种，在这里我们介绍一种方法，就是基于二次求导大于等于0。比如给定一个函数$f(x)=x^2-3x+3$，做两次\n",
    "求导之后即可以得出$f''(x)=2 > 0$，所以这个函数就是凸函数。类似的，这种理论也应用于多元变量中的函数上。在多元函数上，只要证明二阶导数是posititive semidefinite即可以。 问题（c）的结果是一个矩阵。 为了证明这个矩阵（假设为H)为Positive Semidefinite，需要证明对于任意一个非零向量$v\\in \\mathcal{R}$, 需要得出$v^{T}Hv >=0$\n",
    "请写出详细的推导过程："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "// TODO 请写下推导过程<br>\n",
    "\n",
    "易知，$domL(w)$是凸集<br>\n",
    "由1.1.3可得：<br>\n",
    "$\\frac{\\partial^2 L(w)}{\\partial^2 w}=\\sum_{i=1}^n\\frac{e^{-\\vec{w}^T\\vec{x_i}}}{(1+e^{-\\vec{w}^T\\vec{x_i}})^2}\\vec{x_i}\\vec{x_i}^T$<br>\n",
    "即：<br>\n",
    "$\\nabla^2L(w)=\\sum_{i=1}^n\\frac{e^{-\\vec{w}^T\\vec{x_i}}}{(1+e^{-\\vec{w}^T\\vec{x_i}})^2}\\vec{x_i}\\vec{x_i}^T$<br>\n",
    "\n",
    "设$\\vec{x_i}\\in{R^m},i\\in[1,2,\\ldots,n]$，对于$\\forall{\\vec{v}}\\in{R^m}$:<br>\n",
    "$\\vec{v}^T\\vec{x_i}\\vec{x_i}^T\\vec{v} = (\\vec{x_i}^T\\vec{v})^T\\vec{x_i}^T\\vec{v}\\ge0, ,i\\in[1,2,\\ldots,n]$<br>\n",
    "则有:<br>\n",
    "$\\sum_{i=1}^n\\vec{v}^T\\vec{x_i}\\vec{x_i}^T\\vec{v}\\ge0$<br>\n",
    "$\\Rightarrow\\vec{v}^T(\\sum_{i=1}^n\\vec{v}^T\\vec{x_i}\\vec{x_i}^T)\\vec{v}\\ge0$<br>\n",
    "$\\Rightarrow\\vec{v}^T(\\sum_{i=1}^n\\frac{e^{-\\vec{w}^T\\vec{x_i}}}{(1+e^{-\\vec{w}^T\\vec{x_i}})^2}\\vec{v}^T\\vec{x_i}\\vec{x_i}^T)\\vec{v}\\ge0$<br>\n",
    "$\\Rightarrow\\sum_{i=1}^n\\frac{e^{-\\vec{w}^T\\vec{x_i}}}{(1+e^{-\\vec{w}^T\\vec{x_i}})^2}\\vec{x_i}\\vec{x_i}^T\\underline{\\succ}0$<br>\n",
    "\n",
    "即，$\\nabla^2L(w)$半正定<br>\n",
    "说明逻辑回归的目标函数$L(w)$是一个凸函数\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 问题2\n",
    "证明p-norm是凸函数， p-norm的定义为：\n",
    "$||x||_p=(\\sum_{i=1}^{n}|x_i|^p)^{1/p}$\n",
    "\n",
    "hint: Minkowski’s Inequality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "// TODO: your proof<br>\n",
    "考虑到函数的可微性，这里假设所有的$x_i\\ge0$<br>\n",
    "令$f(\\vec{x})=||\\vec{x}||_p=(\\sum_{i=1}^{n}|x_i|^p)^{1/p}=(\\sum_{i=1}^{n}x_i^p)^{1/p}$\n",
    "\n",
    "$\\frac{\\partial f(\\vec{x})}{\\partial x_i}=(\\sum_{j=1}^{n}{x_j}^p)^{\\frac{1-p}{p}}{x_i}^{p-1}$<br>\n",
    "$\\Rightarrow$\n",
    "\n",
    "$\\frac{\\partial^2 f(\\vec{x})}{\\partial x_i^2}=(1-p)(\\sum_{k=1}^{n}{x_k}^p)^{\\frac{1}{p}}\\frac{x_i^{2p-2}-(\\sum_{k=1}^{n}{x_k}^p)x_i^{p-2}}{(\\sum_{k=1}^{n}{x_k}^p)^2}$\n",
    "\n",
    "$\\frac{\\partial^2 f(\\vec{x})}{\\partial x_i\\partial x_j}=(1-p)(\\sum_{k=1}^{n}{x_k}^p)^{\\frac{1}{p}}\\frac{x_i^{p-1}x_j^{p-1}}{(\\sum_{k=1}^{n}{x_k}^p)^2}$\n",
    "\n",
    "写出$f(\\vec{x})$的Hessian矩阵：<br>\n",
    "$\\nabla f(\\vec{x}) = \n",
    " \\left[\n",
    " \\begin{matrix}\n",
    "   \\frac{\\partial^2 f(\\vec{x})}{\\partial x_1^2} & \\frac{\\partial^2 f(\\vec{x})}{\\partial x_1\\partial x_2} & \\cdots & \\frac{\\partial^2 f(\\vec{x})}{\\partial x_1\\partial x_n} \\\\\n",
    "   \\frac{\\partial^2 f(\\vec{x})}{\\partial x_2\\partial x_1} & \\frac{\\partial^2 f(\\vec{x})}{\\partial x_2^2} & \\cdots & \\frac{\\partial^2 f(\\vec{x})}{\\partial x_2\\partial x_n} \\\\\n",
    "   \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "   \\frac{\\partial^2 f(\\vec{x})}{\\partial x_n\\partial x_1} & \\frac{\\partial^2 f(\\vec{x})}{\\partial x_n\\partial x_2} & \\cdots & \\frac{\\partial^2 f(\\vec{x})}{\\partial x_n^2}\n",
    "  \\end{matrix}\n",
    "  \\right]\n",
    "  = \\frac{(1-p)(\\sum_{k=1}^{n}{x_k}^p)^{\\frac{1}{p}}}{(\\sum_{k=1}^{n}{x_k}^p)^2}\n",
    " \\left[\n",
    " \\begin{matrix}\n",
    "   x_1^{2p-2}-(\\sum_{k=1}^{n}{x_k}^p)x_1^{p-2} & x_1^{p-1}x_2^{p-1} & \\cdots & x_1^{p-1}x_n^{p-1} \\\\\n",
    "   x_2^{p-1}x_1^{p-1} & x_2^{2p-2}-(\\sum_{k=1}^{n}{x_k}^p)x_2^{p-2} & \\cdots & x_2^{p-1}x_n^{p-1} \\\\\n",
    "   \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "   x_n^{p-1}x_1^{p-1} & x_n^{p-1}x_2^{p-1} & \\cdots & x_n^{2p-2}-(\\sum_{k=1}^{n}{x_k}^p)x_n^{p-2} \n",
    "  \\end{matrix}\n",
    "  \\right]\n",
    "$\n",
    "\n",
    "记 $\\vec{z}=[x_1^p,x_2^p,\\cdots,x_n^p]^T；A=diag([\\frac{1}{x_1},\\frac{1}{x_2},\\cdots,\\frac{1}{x_n}])$，则$\\nabla^2f(\\vec{x})$可分解为\n",
    "\n",
    "$\\nabla^2f(\\vec{x})=\\frac{(1-p)(\\sum_{k=1}^{n}{x_k}^p)^{\\frac{1}{p}}}{(\\sum_{k=1}^{n}{x_k}^p)^2}A^T[\\vec{z}\\vec{z}^T-(\\vec{1}^T\\vec{z})diag(\\vec{z})]A$\n",
    "\n",
    "对于$\\forall{\\vec{v}}\\in{R^m}$，记$\\vec{g}=A\\vec{v}$，则有：\n",
    "\n",
    "$\\begin{eqnarray}\n",
    "\\vec{v}^T\\nabla^2f(\\vec{x})\\vec{v} & = & \\frac{(1-p)(\\sum_{k=1}^{n}{x_k}^p)^{\\frac{1}{p}}}{(\\sum_{k=1}^{n}{x_k}^p)^2}\\vec{g}^T[\\vec{z}\\vec{z}^T-(\\vec{1}^T\\vec{z})diag(\\vec{z})]\\vec{g} \\\\\n",
    "& = & \\frac{(1-p)(\\sum_{k=1}^{n}{x_k}^p)^{\\frac{1}{p}}}{(\\sum_{k=1}^{n}{x_k}^p)^2}[\\vec{g}^T\\vec{z}\\vec{z}^T\\vec{g}-(\\vec{1}^T\\vec{z})\\vec{g}^Tdiag(\\vec{z})\\vec{g}]\\\\\n",
    "& = & \\frac{(1-p)(\\sum_{k=1}^{n}{x_k}^p)^{\\frac{1}{p}}}{(\\sum_{k=1}^{n}{x_k}^p)^2}[(\\sum_{k=1}^ng_iz_i)^2-(\\sum_{k=1}^nz_i)(\\sum_{k=1}^ng_i^2z_i)]\n",
    "\\end{eqnarray}\n",
    "$\n",
    "\n",
    "令$a_i=g_i\\sqrt{z_i},b_i=\\sqrt{z_i}$，则有\n",
    "\n",
    "$\\vec{v}^T\\nabla^2f(\\vec{x})\\vec{v} = \\frac{(1-p)(\\sum_{k=1}^{n}{x_k}^p)^{\\frac{1}{p}}}{(\\sum_{k=1}^{n}{x_k}^p)^2}[(\\sum_{k=1}^na_ib_i)^2-(\\sum_{k=1}^nb_i^2)(\\sum_{k=1}^na_i^2)]$\n",
    "\n",
    "由柯西不等式得：$(\\sum_{k=1}^na_ib_i)^2-(\\sum_{k=1}^nb_i^2)(\\sum_{k=1}^na_i^2)\\le0$，则\n",
    "\n",
    "当$p\\ge1$时，$\\vec{v}^T\\nabla^2f(\\vec{x})\\vec{v}\\ge0$；当$p\\lt1$时，$\\vec{v}^T\\nabla^2f(\\vec{x})\\vec{v}\\le0$，即\n",
    "\n",
    "$\\begin{cases}\n",
    "\\nabla^2f(\\vec{x})\\underline{\\succ}0 \\qquad & p \\ge 1 \\\\\n",
    "\\nabla^2f(\\vec{x})\\underline{\\prec}0 \\qquad & p \\lt 1\n",
    "\\end{cases}$\n",
    "\n",
    "即：<br>\n",
    "$\\begin{cases}\n",
    "||\\vec{x}||_p是凸函数 \\qquad & p \\ge 1 \\\\\n",
    "||\\vec{x}||_p是凹函数 \\qquad & p \\lt 1\n",
    "\\end{cases}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 问题3\n",
    "在课堂里我们讲过Transportation problem. 重新描述问题： 有两个城市北京和上海，分别拥有300件衣服和500件衣服，另外有三个城市分别是1，2，3分别需要200，300，250件衣服。现在需要把衣服从北京和上海运送到城市1，2，3。 我们假定每运输一件衣服会产生一些代价，比如：\n",
    "- 北京 -> 1:  5\n",
    "- 北京 -> 2:  6\n",
    "- 北京 -> 3:  4\n",
    "- 上海 -> 1:  6\n",
    "- 上海 -> 2:  3\n",
    "- 上海 -> 3:  7\n",
    "\n",
    "最后的值是单位cost. \n",
    "\n",
    "问题：我们希望最小化成本。 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```(a)``` 请写出linear programming formulation。 利用标准的写法(Stanford form)，建议使用矩阵、向量的表示法。 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "// your formulation\n",
    "\n",
    "设从北京运到1、2、3的衣服数目分别为$t_1, t_2, t_3$，上海运到1、2、3的衣服数目分别为$t_4,t_5,t_6$<br>\n",
    "则最小化成本问题为以下形式：\n",
    "\n",
    "$\\begin{eqnarray}\n",
    "minimize & \\qquad5t_1+6t_2+4t_3+6t_4+3t_5+7t+6 \\\\\n",
    "s.t. & t_1+t_2+t_3\\le500 \\\\\n",
    " & t_4+t_5+t_6\\le300 \\\\\n",
    " & t_1+t_4=200 \\\\\n",
    " & t_2+t_5=300 \\\\\n",
    " & t_3+t_6=250 \\\\\n",
    " & t_1,t_2,t_3,t_4,t_5,t_6\\ge0\n",
    " \\end{eqnarray}\n",
    "$\n",
    "\n",
    "使用向量与矩阵的表示方法：<br>\n",
    "记$\\vec{t}=[t_1,t_2,t_3,t_4,t_5,t_6]^T；\\vec{c}=[5,6,4,6,3,7]^T$<br>\n",
    "$A_{eq}=\n",
    "\\left[\n",
    "\\begin{matrix}\n",
    "1 & 0 & 0 & 1 & 0 & 0 \\\\\n",
    "0 & 1 & 0 & 0 & 1 & 0 \\\\\n",
    "0 & 0 & 1 & 0 & 0 & 1 \\\\\n",
    "\\end{matrix}\n",
    "\\right]；\n",
    "\\vec{a_{ub1}}=[1,1,1,0,0,0]^T；\\vec{a_{ub2}}=[0,0,0,1,1,1]^T$<br>\n",
    "将优化问题重写成下面这种形式：\n",
    "\n",
    "$\\begin{eqnarray}\n",
    " minimize & \\qquad \\vec{c}^T\\vec{t} \\\\\n",
    " s.t. & \\vec{a_{ub1}}^T\\vec{t}\\le500 \\\\\n",
    "  & \\vec{a_{ub2}}^T\\vec{t}\\le300 \\\\\n",
    "  & A_{eq}\\vec{t}=[200,300,250]^T \\\\\n",
    "  & t_1,t_2,t_3,t_4,t_5,t_6\\ge0\n",
    " \\end{eqnarray}$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```(b)``` 利用lp solver求解最优解。 参考：\n",
    "https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.linprog.html\n",
    "    或者： http://cvxopt.org/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "最佳的运输方案是：\n",
      "北京向城市1运输：200件\n",
      "北京向城市2运输：0件\n",
      "北京向城市3运输：250件\n",
      "上海向城市1运输：0件\n",
      "上海向城市2运输：300件\n",
      "上海向城市3运输：0件\n",
      "最小运输成本是2900cost\n",
      "3850\n"
     ]
    }
   ],
   "source": [
    "# your implementation\n",
    "from scipy.optimize import linprog\n",
    "import numpy as np\n",
    "import math\n",
    "c = [5,6,4,6,3,7]\n",
    "A1 = [[1,1,1,0,0,0], [0,0,0,1,1,1]]\n",
    "b1 = [500,300]\n",
    "A2 = [[1,0,0,1,0,0], [0,1,0,0,1,0], [0,0,1,0,0,1]]\n",
    "b2 = [200,300,250]\n",
    "bounds_all = [(0,None) for _ in range(6)]\n",
    "res = linprog(c, A_ub=A1, b_ub=b1, A_eq=A2, b_eq=b2, bounds=bounds_all)\n",
    "print('最佳的运输方案是：')\n",
    "result = []\n",
    "for i in range(6):\n",
    "    if res.x[i]<1:\n",
    "        r = 0\n",
    "    else:\n",
    "        r = math.ceil(res.x[i])\n",
    "    result.append(r)\n",
    "    if i<3:\n",
    "        print('北京向城市%d运输：%d件'%(i+1,r))\n",
    "    else:\n",
    "        print('上海向城市%d运输：%d件'%(i-2,r))\n",
    "print('最小运输成本是%dcost'%(np.dot(c,result)))\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```(c)```: 试着把上述LP转化成Dual formulation，请写出dual form. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "// your formulation\n",
    "\n",
    "$\\begin{eqnarray} \n",
    "maximize & \\qquad-500{\\lambda}_1-300{\\lambda}_2-200{\\nu}_1-300{\\nu}_2-250{\\nu}_3 \\\\\n",
    "    s.t. & {\\lambda}_i+{\\nu}_j \\ge 0 \\qquad i\\in{1,2},j\\in{1,2,3}\n",
    "\\end{eqnarray}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
